{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just another \"test\" sent by NowTV which I share here for the benefit of mankind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Problem\n",
    "======\n",
    "\n",
    "Word puzzle solver\n",
    "-------\n",
    "\n",
    "Some newspapers have a word problem where nine characters are displayed and you form as many words as possible from these letters.  You must use the highlighted letter.\n",
    "\n",
    "For example:\n",
    "\n",
    "http://nineletterword.tompaton.com/\n",
    "\n",
    " \n",
    "\n",
    "For our problem, you are given a nine character string and need to return a list of strings containing all words you can form, which must also contain the first character of the input string.  The words should check against an online word list such as:\n",
    "\n",
    "https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
    "\n",
    " \n",
    "\n",
    "The starting method signature is:\n",
    "\n",
    " \n",
    "```scala\n",
    "def solutions(in: String): List[String] = ???\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution\n",
    "====\n",
    "\n",
    "The solution is presented here as a Jupyter notebook, aiming validade the conceptual design before any effort is spent implementing production code and its test suite. This way, we avoid rework which happens in general when a clear conceptual design is not known before hand. Rework costs money and consumes time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary words\n",
    "------------------\n",
    "This is a dataset consisting of 350,000+ words apparently collected from a number of documents not known at this time. Several words are not really words but parts of words such as *'s*, *'ll*, *'mongst*. Also, there are numbers like *2*, *1080*, *3rd*. Also, there are expressions like *ahh*. So, a data cleansing would be probably a good idea here.\n",
    "\n",
    "However, for the sake of this exercise, we simply consider all entries of the dictionary as valid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val source = scala.io.Source.fromURL(\"https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtables and prime numbers\n",
    "--------------------------------\n",
    "In order to expedite access to the dictionary, we calculate a hash code for a given word and we insert that word into a balanced tree.\n",
    "\n",
    "Given that there are 350,000+ words in the dictionary, we've chosen a prime number so that we expect that each balanced tree will have up to 20 elements or so, which means that we can arrive to the word we are looking for in up to 5 comparisons, supported by a tree of up to 32 elements.\n",
    "\n",
    "There's no really guarantee that all trees will have up to 32 elements. This is just an expectation which may or may not turn to be true. After building the trees we check what would be the most populated tree, in order to double check if the prime number we've chosen seems to be a reasonable choice.\n",
    "\n",
    "Hashing and hashing again\n",
    "-----------------------------\n",
    "To be more precise, we find the ``hashCode`` (let's call it simply ``hash``), we divide it by a prime number, finding a another hash which is more manageable (let's call it ``tinyHash``).\n",
    "\n",
    "```scala\n",
    "val hash     = word.hashCode\n",
    "val tinyHash = hash % prime\n",
    "```\n",
    "\n",
    "This is done this way since we will keep a reasonably small data structure indexed by ``tinyHash``. Each entry in this data structure is a balanced tree which contains words hashed by their true hash code, i.e.: ``hash``. This way we try to avoid collisions.\n",
    "\n",
    "Balanced trees and binary search\n",
    "-------------------------------------\n",
    "\n",
    "Despite our efforts to avoid collisions in the previous step, there's no really guarantee that we will definitely avoid collisions at all times. If we fail to do that, the last update wins, which means that only the last word inserted will be present in the tree, since it overwrites all other words with the very same hash code.\n",
    "\n",
    "How we can circumvent this problem?\n",
    "\n",
    "One answer would be employing a balanced tree which supports duplicates. Another way would be employing some other data structure which behaves well in the presence of duplicates.\n",
    "\n",
    "So, instead of a balance tree, we will be simply storing an ordered list of words. Given that this data structure is ordered, we can then employ a **binary search** in order to arrive to a possible match. The number of comparisons of a binary search is (not by coincidence!) the same number of comparisons we would observe if we were employing a balanced tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val prime = 16381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines: Seq[String] = source.getLines.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dict = lines.groupBy(line => (line.hashCode%prime).abs).mapValues(_.sorted.toArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying tree depth\n",
    "-----------------------\n",
    "We've chosen the prime number so that we would expect up to 5 comparisons, i.e.: a tree with up to 32 elements. Lets verify what would be the maximum number of elements we've got. If it is less than or equal to 32, we are doing things according to plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict.mapValues(_.size).map{ case (k,v) => v }.reduceLeft(_ max _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit off track\n",
    "-----------------\n",
    "The maximum tree depth in this case implies on a maximum of 6 comparisons. A bit higher than we would expect but still not bad at all.\n",
    "\n",
    "Given any word, we find its *conceptual* balanced tree (actually an array which we find information employing a binary search) in constant time. After that, we do a maximum of 6 comparisons. It's not bad at all.\n",
    "\n",
    "Obviously we can make it better if performance happens to be an issue. In order to do that, we need to try other prime numbers. I will let this exercise for the reader.\n",
    "\n",
    "Ability to find words\n",
    "---------------------\n",
    "OK. Now we need the ability to find words in the dictionary.\n",
    "\n",
    "Given a certain ``String``, we need to find its ``hash`` and its ``tinyHash``. The ``tinyHash`` is employed so that we can find the *conceptual* balanced tree (implemented as an ordered ``Array[String]``). Found that, we then perform a binary search.\n",
    "\n",
    "The binary search will find words which match the ``String`` we have at hand, in case there's such word in the dictionary. We will not use ``hash`` directly, despite that the algorithm which performs the binary search may or may not employ the very same concept.\n",
    "\n",
    "OK. First of all, we need to wrap ``java.util.Array#binarySearch`` into something a bit more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RichArray[T <: AnyRef](a: Array[T]) { \n",
    "   def binarySearch(key: T) = {\n",
    "     java.util.Arrays.binarySearch(a.asInstanceOf[Array[AnyRef]],key)\n",
    "   }\n",
    "}\n",
    "implicit def richArray[T <: AnyRef](a: Array[T]) = new RichArray(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding one word or two\n",
    "-----------------------\n",
    "Now let's exercise the idea.\n",
    "\n",
    "Given a certain ``String``, we find the *conceptual balanced tree* (which is actually implemented as an ordered ``Array[String]``) and we perform a binary search.\n",
    "\n",
    "If we find a positive index, it's in the dictionary. If we find a negative index, it's not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    val word = \"resilient\"\n",
    "    val tinyHash = (word.hashCode % prime).abs\n",
    "    val index = dict(tinyHash).binarySearch(word)\n",
    "    val found = index >= 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    val word = \"linux\"\n",
    "    val tinyHash = (word.hashCode % prime).abs\n",
    "    val index = dict(tinyHash).binarySearch(word)\n",
    "    val found = index >= 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sizing the problem\n",
    "---------------------\n",
    "\n",
    "OK. Now that we know that we can find words in the dictionary limited by a higher boundary of 6 comparisons, it's time to think about how we can find the words from the puzzle in the dictionary.\n",
    "\n",
    "The problem is: we don't have words in the puzzle: we have just a certain quantity of letters which may be eventually a dictionary word. This is not really a problem, since we can determine if a candidate string is a dictionary word in just 6 comparisons. The problem is the size of the problem.\n",
    "\n",
    "The problem is that we have up to 9 letters as a candidate word and we can shuffle these letters any way we wish; we can also remove letters and shuffle again. The problem is that the size of the problem is *roughly* P(9,8) + P(9,7) + ... + P(9,3) + P(9,2) where P(n,m) represents the permutation of *n* letters grouped by *m*. This is a big number. P(9,8) is ~43 million... so we don't even need to finish the entire calculation to realize that performing these sort of permutations **is not** the way to go.\n",
    "\n",
    "\n",
    "A tentative approach\n",
    "-----------------------\n",
    "What if we do not make any permutations at all? We could simply consider a certain set of letters, regardless their relative order.\n",
    "\n",
    "Now the problem reduces to the ability to find in the dictionary those words which share the same properties of that set of letters we have at hand, regardless their relative order.\n",
    "\n",
    "We could also try to reduce the problem given the number of letters we have. Since we've selected 5 letters, we can be sure that dictionary words made of 4 letters are not good candidates.\n",
    "\n",
    "OK. The idea is: let's calculate a relatively naive hash function and attach information about the number of letters we are interested. For example (and simplistically):\n",
    "\n",
    "```scala\n",
    "val naiveHash = s(0) + s(1) + ... + s(n-1)\n",
    "val naiveHashPlusSize = naiveHash * 10 + (n%10)\n",
    "```\n",
    "\n",
    "OK. Now we have to calculate ``naiveHashPlusSize`` for every word in the dictionary and create another data structure which classfies words according to this criteria.\n",
    "\n",
    "\n",
    "Refining our plan\n",
    "--------------------\n",
    "\n",
    "So, the idea is now calculate the ``naiveHashPlusSize`` for a candidate word from the puzzle and find a list of dictionary words which match the same ``naiveHashPlusSize``. Sounds good. But, how many dictionary words we are really talking about?\n",
    "\n",
    "Well... it depends on the number of entries in the hashtable and their statistic distribution. We don't really know this information at this point. Let's simply try this idea and see if we obtain a data structure which looks to be reasonable, in other words: there's a relatively manageable number of words sharing the same ``naiveHashPlusSize``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveHashPlusSize(s: String): Int = {\n",
    "    (s.map(c => c - ' ').sum * 10) + (s.length % 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dict2 = lines.groupBy(line => naiveHashPlusSize(line)).mapValues(_.sorted.toArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2.mapValues(_.size).map{ case (k,v) => v }.reduceLeft(_ max _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks relatively well\n",
    "---------------------\n",
    "In a nutshell, it means that we will do a maximum of ~1100 comparisons in the worst case.\n",
    "\n",
    "This is definitely better than a full table scan as per\n",
    "https://github.com/dwyl/autocomplete/blob/master/index.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More refinements?\n",
    "-----------------\n",
    "\n",
    "There's definitely room for more refinements.\n",
    "\n",
    "The way it is at the moment, for every word in our data structure (here called ``dict2``) we will have to compare if shuffling this word eventually arrives to the candidate word we have at hand.\n",
    "\n",
    "Actually, it's easier to do something different: we sort the candidate word we have at hand and we sort the word obtained from ``dict2`` and we see whether they match or not. Something like this:\n",
    "\n",
    "```scala\n",
    "if(candidate.sorted == word.sorted) ... // we've found something here!\n",
    "```\n",
    "\n",
    "We will have to do this ~1100 times in the worst case, every time a new candidate word is entered. It would be nice if we had a binary search here too. Employing a binary search, we reduce the number of comparisons from ~1100 to only ~10 comparisons.\n",
    "\n",
    "Given a candidate word, we sort its component letters and we try to find it in the hashtable which, not by coincidence, must have dictionary words already sorted too.\n",
    "\n",
    "To be more precise, we actually have to keep both: we need to find dictionary words via its sorted representation and, after that, we need to return the original representation, as plain text, exactly as provided in the input source.\n",
    "\n",
    "Since dictionary words may collide after sorted, we need to actually store a hashtable inside a hashtable. There's simply no way to escape this fact, even if we find a bigger prime number as divisor, even if we do not divide the calculated hash by any prime number at all.\n",
    "\n",
    "The data structure consists of a hashtable or hashtables, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq(\"faca\", \"cafa\", \"jacu\", \"cuja\", \"abb\", \"bba\", \"aba\", \"aabb\", \"abba\", \"bbaa\")\n",
    "    .groupBy(line => line.hashCode.abs%2)\n",
    "    .mapValues(words => \n",
    "                 words\n",
    "                   .map(word => (word.sorted -> word))\n",
    "                   .groupBy(_._1).map { case (k,v) => (k,v.map(_._2))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define ``dict3``, which hopefully is our final version of the most important data structure we need to solve this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dict3 =\n",
    "  lines\n",
    "    .groupBy(line => naiveHashPlusSize(line))\n",
    "    .mapValues(words => \n",
    "                 words\n",
    "                   .map(word => (word.sorted -> word))\n",
    "                   .groupBy(_._1).map { case (k,v) => (k,v.map(_._2))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ability to match candidate words\n",
    "-------------------------------------\n",
    "\n",
    "OK. Now we are ready to enter some sort of random text and see if we find dictionary words for it. Let's try a couple of words and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWords(candidate: String): Seq[String] = {\n",
    "    val hash = naiveHashPlusSize(candidate)\n",
    "    val sorted = candidate.sorted\n",
    "    val matches = \n",
    "      dict3\n",
    "        .getOrElse(hash, Map.empty[String, Seq[String]])\n",
    "        .getOrElse(sorted, Seq.empty[String])\n",
    "        .filter(word => sorted == word.sorted)\n",
    "    matches\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findWords(\"drst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findWords(\"evarega\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findWords(\"resliient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ability to find all candidate substrings of candidate word\n",
    "------------------------------------------------------------------\n",
    "\n",
    "Now, all we need to do is the ability to find all substrings of a candidate word, not forgetting that the **first letter** must be always present.\n",
    "\n",
    "The idea is that we find all substrings of a candidate word *except the first letter*, then we add the first letter later to all positions it would be necessary.\n",
    "\n",
    "But wait! We will sort the candidate word (or candidate substring of it) anyway. So, it does not matter. We can simply add the first letter to the beginning and we are done. Also, we don't need to care about relative order of characters, since we are going to sort letters anyway, the same way we sort dictionary words when we insert them into ``dict3``.\n",
    "\n",
    "So, below we demonstrate how it would work. Suppose the word \"darts\". We remove \"d\" and we obtain a list of substrings from \"arts\", like shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts(s: String): Seq[String] = {\n",
    "    s.size match {\n",
    "        case 0 => Seq.empty[String]\n",
    "        case 1 => Seq(s(0).toString)\n",
    "        case _ => \n",
    "            s.substring(1).inits.flatMap(_.tails.toList.init).toSeq\n",
    "              .map(text => s(0) + text)\n",
    "              .toSet\n",
    "              .toList\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts(\"darts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Antidisestablishmentarianism\".toLowerCase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together\n",
    "-----------------------\n",
    "\n",
    "Now we are ready to arriving to a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solutions(in: String): Seq[String] = {\n",
    "    parts(in.toLowerCase).flatMap(candidate => findWords(candidate))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a couple of performance tests\n",
    "-----------------------------------\n",
    "Let's employ 9 characters as the specification says. But let's also play with a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(letters: String): (Seq[String], Long) = {\n",
    "    val before = new java.util.Date().getTime()\n",
    "    val result = solutions(letters)\n",
    "    val after  = new java.util.Date().getTime()\n",
    "    val elapsed = after - before\n",
    "    (result, elapsed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    val (result, milliseconds) = test(\"aimlessly\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    val (result, milliseconds) = test(\"confirmed\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    val (result, milliseconds) = test(\"performance\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    val (result, milliseconds) = test(\"development\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    val (result, milliseconds) = test(\"Antidisestablishmentarianism\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Access time around ~20ms for candidate words of 9 letters looks pretty good.\n",
    "\n",
    "The first part of the exploration was not really used in the final solution, but helped as an explorarion of the problem domain and, in particular, in regards to performance issues.\n",
    "\n",
    "But... if the input source was already sorted (or at least apparently sorted or apparently partially sorted)... why then creating expending extra time loading a relatively complex data structure? Couldn't the idea of a binary search be applicable straight away to a large ``Array[String]`` which contains all dictionary words?\n",
    "\n",
    "The short answer is: if you are willing to perform just a couple of queries... yes. However, if you are willing to provide a service which performs well under load then, in this case, performance is key and every millisecond counts.\n",
    "\n",
    "And there's still more room for optimization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
