{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Docker image, if needed\n",
    "\n",
    "The easiest way to deploy Spark (and optionally Mesos) onto a laptop is\n",
    "running Docker image ``spark_mesos``, which we build and run in the\n",
    "instructions below.\n",
    "\n",
    "These are the steps which automate the entire procedure:\n",
    "\n",
    "```bash\n",
    "$ sudo apt-get install git\n",
    "\n",
    "$ mkdir -p $HOME/workspace && cd $HOME/workspace\n",
    "$ git clone http://github.com/frgomes/debian-scripts\n",
    "\n",
    "$ cd debian-scripts\n",
    "$ ./install-docker-spark+mesos.sh\n",
    "```\n",
    "\n",
    "These steps will create a Docker image named ``spark_mesos`` and\n",
    "will create a shell script which automates the startup of that.\n",
    "\n",
    "```bash\n",
    "$ docker images spark_mesos\n",
    "REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n",
    "spark_mesos         latest              acc97c716500        14 hours ago        1.09GB\n",
    "\n",
    "$ ls -al /opt/bin/spark_mesos.sh \n",
    "-rwxr-xr-x 1 rgomes rgomes 226 Jun  4 01:50 /opt/bin/spark_mesos.sh\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "### Notes on Mesos - Part 1\n",
    "\n",
    "In order to access the Mesos controller running in the Docker container, we need\n",
    "to install Mesos in our laptop. However, installing Mesos in our laptop is outside\n",
    "of the aims of this exercise.\n",
    "\n",
    "You will see that we obtain a ``SparkSession`` Spark port 7077, exposed by the running\n",
    "Docker container. The code looks like this:\n",
    "\n",
    "```scala\n",
    "val ss: SparkSession = \n",
    "  JupyterSparkSession\n",
    "    .builder() \n",
    "    .jupyter()\n",
    "    .master(\"local[4]\").config(\"spark.ui.port\",\"7077\")\n",
    "    .appName(\"fraud-detection\")\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Notes on Mesos - Part 2\n",
    "\n",
    "If you are adventurous to employ Mesos, you could probably substitute...\n",
    "```scala\n",
    ".master(\"local[4]\").config(\"spark.ui.port\",\"7077\")\n",
    "```\n",
    "by...\n",
    "```scala\n",
    ".master(\"mesos://localhost:5050\")\n",
    ".config(\"java.library.path\",\n",
    "            \"/usr/local/lib/libmesos.so\")\n",
    ".config(\"spark.executor.uri\",\n",
    "            \"http://www.apache.org/dyn/closer.lua/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz\")\n",
    "```\n",
    "Well... probably. This code was never tested.\n",
    "\n",
    "### Notes on Mesos - Part 3\n",
    "\n",
    "Chances are that, in future, we will be able to manage Mesos via a RESTful API, instead of having\n",
    "to go thru a painful process of downloading the sources, building the binaries and installing\n",
    "Mesos onto our laptop.\n",
    "\n",
    "More info: http://github.com/mesosphere/mesos-http-adapter\n",
    "\n",
    "----\n",
    "\n",
    "### Run Docker image\n",
    "\n",
    "Supposing you've built a Docker image as explained above, all you have to do is starting it, so that Spark and Mesos will be available easily. It's just a matter of running the command below:\n",
    "```bash\n",
    "$ /opt/bin/spark_mesos.sh\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "### Import magic for Spark 2.2.1\n",
    "See available packages here: http://central.maven.org/maven2/org/apache/spark/\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                           \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "//import $ivy.`org.apache.spark::spark-mesos:2.2.1`\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.catalyst.expressions.aggregate._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21`\n",
    "\n",
    "import $profile.`hadoop-2.7`\n",
    "import $ivy.`org.apache.spark::spark-core:2.2.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.2.1`\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.2.1`\n",
    "//import $ivy.`org.apache.spark::spark-mesos:2.2.1`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2`\n",
    "\n",
    "import jupyter.spark.session._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.catalyst.expressions.aggregate._\n",
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1e772751\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@4c0a21b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark: SparkSession = \n",
    "  JupyterSparkSession\n",
    "    .builder() \n",
    "    .jupyter()\n",
    "    .master(\"local[4]\").config(\"spark.ui.port\",\"7077\")\n",
    "    //.master(\"mesos://localhost:5050\").config(\"spark.mesos.http.adapter\", true)\n",
    "    .appName(\"csv_read\")\n",
    "//  .config(\"spark.executor.instances\", \"10\")\n",
    "//  .config(\"spark.executor.memory\", \"3g\")\n",
    "    .getOrCreate()\n",
    "\n",
    "val sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcircumventReplScope\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val circumventReplScope = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SQLContext\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@5292171\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [msgId: string, msgType: string ... 27 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "val df = \n",
    "  spark.sqlContext\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \"|\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/gateway/sample_payment.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres20\u001b[39m: \u001b[32mtypes\u001b[39m.\u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  StructField(msgId,StringType,true),\n",
       "  StructField(msgType,StringType,true),\n",
       "  StructField(msgTs,LongType,true),\n",
       "  StructField(extAccId,StringType,true),\n",
       "  StructField(extAccSdsId,IntegerType,true),\n",
       "  StructField(processedTs,LongType,true),\n",
       "  StructField(valueAmount,DoubleType,true),\n",
       "  StructField(drCrMark,StringType,true),\n",
       "  StructField(ccy,StringType,true),\n",
       "  StructField(valueDate,TimestampType,true),\n",
       "  StructField(normalisedPaymentStatus,StringType,true),\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
